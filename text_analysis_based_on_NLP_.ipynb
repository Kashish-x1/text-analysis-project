{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOX4qLRZRI8I2UaImsV2Mdy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kashish-x1/text-analysis-project/blob/main/text_analysis_based_on_NLP_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 nltk"
      ],
      "metadata": {
        "id": "k6rZ-qutk2EN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf853970-388b-4192-edfd-f02174cb227f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')      # Breaks text into sentences and words\n",
        "nltk.download('stopwords')  # Gives us a list of boring words like \"the\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9ZfplMSu6fU",
        "outputId": "d7274698-e694-4c47-c303-874934304204"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uYP92_HwZRp",
        "outputId": "9b89c941-2d51-4543-e902-6c4d52cf11b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input.xlsx\t\t\t\t   Netclan20241021.txt\tNetclan20241028.txt\n",
            "MasterDictionary\t\t\t   Netclan20241022.txt\tNetclan20241029.txt\n",
            "MasterDictionary-20250325T135116Z-001.zip  Netclan20241023.txt\tsample_data\n",
            "Netclan20241017.txt\t\t\t   Netclan20241024.txt\tStopWords\n",
            "Netclan20241018.txt\t\t\t   Netclan20241025.txt\tStopWords-20250325T135103Z-001.zip\n",
            "Netclan20241019.txt\t\t\t   Netclan20241026.txt\n",
            "Netclan20241020.txt\t\t\t   Netclan20241027.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o /content/StopWords-20250325T135103Z-001.zip -d /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etsqUTnQzpFA",
        "outputId": "c899c463-ed1f-465d-d598-340b22483464"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/StopWords-20250325T135103Z-001.zip\n",
            "  inflating: /content/StopWords/StopWords_Generic.txt  \n",
            "  inflating: /content/StopWords/StopWords_Geographic.txt  \n",
            "  inflating: /content/StopWords/StopWords_Auditor.txt  \n",
            "  inflating: /content/StopWords/StopWords_Currencies.txt  \n",
            "  inflating: /content/StopWords/StopWords_Names.txt  \n",
            "  inflating: /content/StopWords/StopWords_GenericLong.txt  \n",
            "  inflating: /content/StopWords/StopWords_DatesandNumbers.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o /content/MasterDictionary-20250325T135116Z-001.zip -d /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr_Vnh0NztqP",
        "outputId": "0e73f2bc-1c5b-4daf-f1d9-ababf188480e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/MasterDictionary-20250325T135116Z-001.zip\n",
            "  inflating: /content/MasterDictionary/negative-words.txt  \n",
            "  inflating: /content/MasterDictionary/positive-words.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd        # Reads Excel\n",
        "import requests           # Downloads webpages\n",
        "from bs4 import BeautifulSoup  # Finds article parts\n",
        "\n",
        "# Load the list of URLs\n",
        "df = pd.read_excel('/content/Input.xlsx')\n",
        "\n",
        "# Go through each of the 147 rows\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']  # Get the ID (e.g., \"Netclan20241017\")\n",
        "    url = row['URL']        # Get the link (e.g., \"https://...\")\n",
        "    try:\n",
        "        # Download the webpage\n",
        "        response = requests.get(url, timeout=10)  # Wait 10 seconds max\n",
        "        response.raise_for_status()  # Make sure it worked\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')  # Understand the webpage\n",
        "\n",
        "        # Find the title\n",
        "        title = soup.find('h1', class_='entry-title')  # Look for <h1 class=\"entry-title\">\n",
        "        if title:  # If found\n",
        "            title_text = title.get_text(strip=True)  # Get just the text\n",
        "        else:\n",
        "            title_text = \"No Title Found\"  # If not found\n",
        "\n",
        "        # Find the article text\n",
        "        content = soup.find('div', class_='td-post-content')  # Look for <div class=\"td-post-content\">\n",
        "        if content:  # If found\n",
        "            paragraphs = content.find_all('p')  # Get all <p> tags inside\n",
        "            article_text = \"\\n\\n\".join(p.get_text(strip=True) for p in paragraphs)  # Combine paragraphs\n",
        "        else:\n",
        "            article_text = \"\"  # If not found\n",
        "\n",
        "        # Put title and text together\n",
        "        full_text = title_text + \"\\n\\n\" + article_text\n",
        "\n",
        "        # Save to a file\n",
        "        with open(f'/content/{url_id}.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(full_text)\n",
        "        print(f\"Saved {url_id}.txt\")\n",
        "    except Exception as e:\n",
        "        # If something goes wrong (e.g., bad link)\n",
        "        print(f\"Error with {url_id}: {e}\")\n",
        "        with open(f'/content/{url_id}.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(\"Error: Could not get content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_E6fPS1xdtS",
        "outputId": "0585e77d-4def-4ff8-80dd-ebb9bbb4a7e9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Netclan20241017.txt\n",
            "Saved Netclan20241018.txt\n",
            "Saved Netclan20241019.txt\n",
            "Saved Netclan20241020.txt\n",
            "Saved Netclan20241021.txt\n",
            "Saved Netclan20241022.txt\n",
            "Saved Netclan20241023.txt\n",
            "Saved Netclan20241024.txt\n",
            "Saved Netclan20241025.txt\n",
            "Saved Netclan20241026.txt\n",
            "Saved Netclan20241027.txt\n",
            "Saved Netclan20241028.txt\n",
            "Saved Netclan20241029.txt\n",
            "Saved Netclan20241030.txt\n",
            "Saved Netclan20241031.txt\n",
            "Saved Netclan20241032.txt\n",
            "Saved Netclan20241033.txt\n",
            "Saved Netclan20241034.txt\n",
            "Saved Netclan20241035.txt\n",
            "Saved Netclan20241036.txt\n",
            "Saved Netclan20241037.txt\n",
            "Saved Netclan20241038.txt\n",
            "Saved Netclan20241039.txt\n",
            "Saved Netclan20241040.txt\n",
            "Saved Netclan20241041.txt\n",
            "Saved Netclan20241042.txt\n",
            "Saved Netclan20241043.txt\n",
            "Saved Netclan20241044.txt\n",
            "Saved Netclan20241045.txt\n",
            "Saved Netclan20241046.txt\n",
            "Saved Netclan20241047.txt\n",
            "Saved Netclan20241048.txt\n",
            "Saved Netclan20241049.txt\n",
            "Saved Netclan20241050.txt\n",
            "Saved Netclan20241051.txt\n",
            "Saved Netclan20241052.txt\n",
            "Saved Netclan20241053.txt\n",
            "Saved Netclan20241054.txt\n",
            "Saved Netclan20241055.txt\n",
            "Saved Netclan20241056.txt\n",
            "Saved Netclan20241057.txt\n",
            "Saved Netclan20241058.txt\n",
            "Saved Netclan20241059.txt\n",
            "Saved Netclan20241060.txt\n",
            "Saved Netclan20241061.txt\n",
            "Saved Netclan20241062.txt\n",
            "Saved Netclan20241063.txt\n",
            "Saved Netclan20241064.txt\n",
            "Saved Netclan20241065.txt\n",
            "Saved Netclan20241066.txt\n",
            "Saved Netclan20241067.txt\n",
            "Saved Netclan20241068.txt\n",
            "Saved Netclan20241069.txt\n",
            "Saved Netclan20241070.txt\n",
            "Saved Netclan20241071.txt\n",
            "Saved Netclan20241072.txt\n",
            "Saved Netclan20241073.txt\n",
            "Saved Netclan20241074.txt\n",
            "Saved Netclan20241075.txt\n",
            "Error with Netclan20241076: HTTPSConnectionPool(host='insights.blackcoffer.com', port=443): Read timed out. (read timeout=10)\n",
            "Saved Netclan20241077.txt\n",
            "Saved Netclan20241078.txt\n",
            "Saved Netclan20241079.txt\n",
            "Saved Netclan20241080.txt\n",
            "Saved Netclan20241081.txt\n",
            "Saved Netclan20241082.txt\n",
            "Saved Netclan20241083.txt\n",
            "Saved Netclan20241084.txt\n",
            "Saved Netclan20241085.txt\n",
            "Saved Netclan20241086.txt\n",
            "Saved Netclan20241087.txt\n",
            "Saved Netclan20241088.txt\n",
            "Saved Netclan20241089.txt\n",
            "Saved Netclan20241090.txt\n",
            "Saved Netclan20241091.txt\n",
            "Saved Netclan20241092.txt\n",
            "Saved Netclan20241093.txt\n",
            "Saved Netclan20241094.txt\n",
            "Saved Netclan20241095.txt\n",
            "Saved Netclan20241096.txt\n",
            "Error with Netclan20241097: HTTPSConnectionPool(host='insights.blackcoffer.com', port=443): Read timed out. (read timeout=10)\n",
            "Saved Netclan20241098.txt\n",
            "Saved Netclan20241099.txt\n",
            "Saved Netclan20241100.txt\n",
            "Saved Netclan20241101.txt\n",
            "Saved Netclan20241102.txt\n",
            "Saved Netclan20241103.txt\n",
            "Saved Netclan20241104.txt\n",
            "Saved Netclan20241105.txt\n",
            "Saved Netclan20241106.txt\n",
            "Saved Netclan20241107.txt\n",
            "Saved Netclan20241108.txt\n",
            "Saved Netclan20241109.txt\n",
            "Saved Netclan20241110.txt\n",
            "Saved Netclan20241111.txt\n",
            "Saved Netclan20241112.txt\n",
            "Saved Netclan20241113.txt\n",
            "Saved Netclan20241114.txt\n",
            "Saved Netclan20241115.txt\n",
            "Saved Netclan20241116.txt\n",
            "Saved Netclan20241117.txt\n",
            "Saved Netclan20241118.txt\n",
            "Saved Netclan20241119.txt\n",
            "Saved Netclan20241120.txt\n",
            "Saved Netclan20241121.txt\n",
            "Saved Netclan20241122.txt\n",
            "Saved Netclan20241123.txt\n",
            "Saved Netclan20241124.txt\n",
            "Saved Netclan20241125.txt\n",
            "Saved Netclan20241126.txt\n",
            "Saved Netclan20241127.txt\n",
            "Saved Netclan20241128.txt\n",
            "Saved Netclan20241129.txt\n",
            "Saved Netclan20241130.txt\n",
            "Saved Netclan20241131.txt\n",
            "Saved Netclan20241132.txt\n",
            "Saved Netclan20241133.txt\n",
            "Saved Netclan20241134.txt\n",
            "Saved Netclan20241135.txt\n",
            "Saved Netclan20241136.txt\n",
            "Saved Netclan20241137.txt\n",
            "Saved Netclan20241138.txt\n",
            "Saved Netclan20241139.txt\n",
            "Saved Netclan20241140.txt\n",
            "Saved Netclan20241141.txt\n",
            "Saved Netclan20241142.txt\n",
            "Saved Netclan20241143.txt\n",
            "Saved Netclan20241144.txt\n",
            "Saved Netclan20241145.txt\n",
            "Saved Netclan20241146.txt\n",
            "Saved Netclan20241147.txt\n",
            "Saved Netclan20241148.txt\n",
            "Saved Netclan20241149.txt\n",
            "Saved Netclan20241150.txt\n",
            "Saved Netclan20241151.txt\n",
            "Saved Netclan20241152.txt\n",
            "Saved Netclan20241153.txt\n",
            "Saved Netclan20241154.txt\n",
            "Saved Netclan20241155.txt\n",
            "Saved Netclan20241156.txt\n",
            "Saved Netclan20241157.txt\n",
            "Saved Netclan20241158.txt\n",
            "Saved Netclan20241159.txt\n",
            "Saved Netclan20241160.txt\n",
            "Saved Netclan20241161.txt\n",
            "Saved Netclan20241162.txt\n",
            "Saved Netclan20241163.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Load the word lists\n",
        "stop_words = set()  # Words to ignore\n",
        "for file in os.listdir('/content/StopWords'):\n",
        "    with open(f'/content/StopWords/{file}', 'r', encoding='latin-1') as f:\n",
        "        stop_words.update(line.strip().lower() for line in f)\n",
        "\n",
        "positive_words = set()  # Happy words\n",
        "negative_words = set()  # Sad words\n",
        "with open('/content/MasterDictionary/positive-words.txt', 'r', encoding='latin-1') as f:\n",
        "    positive_words.update(w.strip().lower() for w in f if w.strip().lower() not in stop_words)\n",
        "with open('/content/MasterDictionary/negative-words.txt', 'r', encoding='latin-1') as f:\n",
        "    negative_words.update(w.strip().lower() for w in f if w.strip().lower() not in stop_words)\n",
        "\n",
        "# Function to analyze one text file\n",
        "def analyze_text(file_path):\n",
        "    # Open and read the file\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "    except:\n",
        "        # If it fails, return all zeros\n",
        "        return {\n",
        "            'POSITIVE SCORE': 0, 'NEGATIVE SCORE': 0, 'POLARITY SCORE': 0, 'SUBJECTIVITY SCORE': 0,\n",
        "            'AVG SENTENCE LENGTH': 0, 'PERCENTAGE OF COMPLEX WORDS': 0, 'FOG INDEX': 0,\n",
        "            'AVG NUMBER OF WORDS PER SENTENCE': 0, 'COMPLEX WORD COUNT': 0, 'WORD COUNT': 0,\n",
        "            'SYLLABLE PER WORD': 0, 'PERSONAL PRONOUNS': 0, 'AVG WORD LENGTH': 0\n",
        "        }\n",
        "\n",
        "    # Break the text into pieces\n",
        "    sentences = sent_tokenize(text)  # List of sentences\n",
        "    words = word_tokenize(text.lower())  # List of words, all lowercase\n",
        "    cleaned_words = [w for w in words if w not in stop_words and w not in string.punctuation]\n",
        "    word_count = len(cleaned_words)  # How many words after cleaning\n",
        "\n",
        "    # Count happy and sad words\n",
        "    positive_score = sum(1 for w in cleaned_words if w in positive_words)\n",
        "    negative_score = sum(1 for w in cleaned_words if w in negative_words)\n",
        "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
        "\n",
        "    # How long are sentences?\n",
        "    total_words = len(words)  # All words, even stop words\n",
        "    sentence_count = len(sentences) if sentences else 1  # Avoid dividing by zero\n",
        "    avg_sentence_length = total_words / sentence_count\n",
        "\n",
        "    # Count syllables in words\n",
        "    def count_syllables(word):\n",
        "        word = word.lower()\n",
        "        if word.endswith(('es', 'ed')):\n",
        "            word = word[:-2]  # Remove \"es\" or \"ed\" endings\n",
        "        vowels = 'aeiouy'\n",
        "        count = sum(1 for char in word if char in vowels)\n",
        "        return max(1, count)  # At least 1 syllable\n",
        "\n",
        "    # Complex words have more than 2 syllables\n",
        "    complex_words = sum(1 for w in cleaned_words if count_syllables(w) > 2)\n",
        "    pct_complex_words = (complex_words / total_words) * 100 if total_words else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + pct_complex_words)\n",
        "    avg_words_per_sentence = total_words / sentence_count\n",
        "    syllable_per_word = sum(count_syllables(w) for w in cleaned_words) / word_count if word_count else 0\n",
        "\n",
        "    # Count personal pronouns\n",
        "    pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I)) - len(re.findall(r'\\bUS\\b', text))\n",
        "\n",
        "    # Average word length\n",
        "    avg_word_length = sum(len(w) for w in cleaned_words) / word_count if word_count else 0\n",
        "\n",
        "    # Return all 13 results\n",
        "    return {\n",
        "        'POSITIVE SCORE': positive_score, 'NEGATIVE SCORE': negative_score,\n",
        "        'POLARITY SCORE': polarity_score, 'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "        'AVG SENTENCE LENGTH': avg_sentence_length, 'PERCENTAGE OF COMPLEX WORDS': pct_complex_words,\n",
        "        'FOG INDEX': fog_index, 'AVG NUMBER OF WORDS PER SENTENCE': avg_words_per_sentence,\n",
        "        'COMPLEX WORD COUNT': complex_words, 'WORD COUNT': word_count,\n",
        "        'SYLLABLE PER WORD': syllable_per_word, 'PERSONAL PRONOUNS': pronouns,\n",
        "        'AVG WORD LENGTH': avg_word_length\n",
        "    }\n",
        "\n",
        "# Analyze all 147 files\n",
        "results = []\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    analysis = analyze_text(f'/content/{url_id}.txt')\n",
        "    result = {'URL_ID': url_id, 'URL': row['URL'], **analysis}\n",
        "    results.append(result)\n",
        "    print(f\"Analyzed {url_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hq1HVRoz3HZ",
        "outputId": "d6409d9a-37d1-4351-9425-2747077849e1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzed Netclan20241017\n",
            "Analyzed Netclan20241018\n",
            "Analyzed Netclan20241019\n",
            "Analyzed Netclan20241020\n",
            "Analyzed Netclan20241021\n",
            "Analyzed Netclan20241022\n",
            "Analyzed Netclan20241023\n",
            "Analyzed Netclan20241024\n",
            "Analyzed Netclan20241025\n",
            "Analyzed Netclan20241026\n",
            "Analyzed Netclan20241027\n",
            "Analyzed Netclan20241028\n",
            "Analyzed Netclan20241029\n",
            "Analyzed Netclan20241030\n",
            "Analyzed Netclan20241031\n",
            "Analyzed Netclan20241032\n",
            "Analyzed Netclan20241033\n",
            "Analyzed Netclan20241034\n",
            "Analyzed Netclan20241035\n",
            "Analyzed Netclan20241036\n",
            "Analyzed Netclan20241037\n",
            "Analyzed Netclan20241038\n",
            "Analyzed Netclan20241039\n",
            "Analyzed Netclan20241040\n",
            "Analyzed Netclan20241041\n",
            "Analyzed Netclan20241042\n",
            "Analyzed Netclan20241043\n",
            "Analyzed Netclan20241044\n",
            "Analyzed Netclan20241045\n",
            "Analyzed Netclan20241046\n",
            "Analyzed Netclan20241047\n",
            "Analyzed Netclan20241048\n",
            "Analyzed Netclan20241049\n",
            "Analyzed Netclan20241050\n",
            "Analyzed Netclan20241051\n",
            "Analyzed Netclan20241052\n",
            "Analyzed Netclan20241053\n",
            "Analyzed Netclan20241054\n",
            "Analyzed Netclan20241055\n",
            "Analyzed Netclan20241056\n",
            "Analyzed Netclan20241057\n",
            "Analyzed Netclan20241058\n",
            "Analyzed Netclan20241059\n",
            "Analyzed Netclan20241060\n",
            "Analyzed Netclan20241061\n",
            "Analyzed Netclan20241062\n",
            "Analyzed Netclan20241063\n",
            "Analyzed Netclan20241064\n",
            "Analyzed Netclan20241065\n",
            "Analyzed Netclan20241066\n",
            "Analyzed Netclan20241067\n",
            "Analyzed Netclan20241068\n",
            "Analyzed Netclan20241069\n",
            "Analyzed Netclan20241070\n",
            "Analyzed Netclan20241071\n",
            "Analyzed Netclan20241072\n",
            "Analyzed Netclan20241073\n",
            "Analyzed Netclan20241074\n",
            "Analyzed Netclan20241075\n",
            "Analyzed Netclan20241076\n",
            "Analyzed Netclan20241077\n",
            "Analyzed Netclan20241078\n",
            "Analyzed Netclan20241079\n",
            "Analyzed Netclan20241080\n",
            "Analyzed Netclan20241081\n",
            "Analyzed Netclan20241082\n",
            "Analyzed Netclan20241083\n",
            "Analyzed Netclan20241084\n",
            "Analyzed Netclan20241085\n",
            "Analyzed Netclan20241086\n",
            "Analyzed Netclan20241087\n",
            "Analyzed Netclan20241088\n",
            "Analyzed Netclan20241089\n",
            "Analyzed Netclan20241090\n",
            "Analyzed Netclan20241091\n",
            "Analyzed Netclan20241092\n",
            "Analyzed Netclan20241093\n",
            "Analyzed Netclan20241094\n",
            "Analyzed Netclan20241095\n",
            "Analyzed Netclan20241096\n",
            "Analyzed Netclan20241097\n",
            "Analyzed Netclan20241098\n",
            "Analyzed Netclan20241099\n",
            "Analyzed Netclan20241100\n",
            "Analyzed Netclan20241101\n",
            "Analyzed Netclan20241102\n",
            "Analyzed Netclan20241103\n",
            "Analyzed Netclan20241104\n",
            "Analyzed Netclan20241105\n",
            "Analyzed Netclan20241106\n",
            "Analyzed Netclan20241107\n",
            "Analyzed Netclan20241108\n",
            "Analyzed Netclan20241109\n",
            "Analyzed Netclan20241110\n",
            "Analyzed Netclan20241111\n",
            "Analyzed Netclan20241112\n",
            "Analyzed Netclan20241113\n",
            "Analyzed Netclan20241114\n",
            "Analyzed Netclan20241115\n",
            "Analyzed Netclan20241116\n",
            "Analyzed Netclan20241117\n",
            "Analyzed Netclan20241118\n",
            "Analyzed Netclan20241119\n",
            "Analyzed Netclan20241120\n",
            "Analyzed Netclan20241121\n",
            "Analyzed Netclan20241122\n",
            "Analyzed Netclan20241123\n",
            "Analyzed Netclan20241124\n",
            "Analyzed Netclan20241125\n",
            "Analyzed Netclan20241126\n",
            "Analyzed Netclan20241127\n",
            "Analyzed Netclan20241128\n",
            "Analyzed Netclan20241129\n",
            "Analyzed Netclan20241130\n",
            "Analyzed Netclan20241131\n",
            "Analyzed Netclan20241132\n",
            "Analyzed Netclan20241133\n",
            "Analyzed Netclan20241134\n",
            "Analyzed Netclan20241135\n",
            "Analyzed Netclan20241136\n",
            "Analyzed Netclan20241137\n",
            "Analyzed Netclan20241138\n",
            "Analyzed Netclan20241139\n",
            "Analyzed Netclan20241140\n",
            "Analyzed Netclan20241141\n",
            "Analyzed Netclan20241142\n",
            "Analyzed Netclan20241143\n",
            "Analyzed Netclan20241144\n",
            "Analyzed Netclan20241145\n",
            "Analyzed Netclan20241146\n",
            "Analyzed Netclan20241147\n",
            "Analyzed Netclan20241148\n",
            "Analyzed Netclan20241149\n",
            "Analyzed Netclan20241150\n",
            "Analyzed Netclan20241151\n",
            "Analyzed Netclan20241152\n",
            "Analyzed Netclan20241153\n",
            "Analyzed Netclan20241154\n",
            "Analyzed Netclan20241155\n",
            "Analyzed Netclan20241156\n",
            "Analyzed Netclan20241157\n",
            "Analyzed Netclan20241158\n",
            "Analyzed Netclan20241159\n",
            "Analyzed Netclan20241160\n",
            "Analyzed Netclan20241161\n",
            "Analyzed Netclan20241162\n",
            "Analyzed Netclan20241163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to CSV\n",
        "output_df = pd.DataFrame(results)\n",
        "output_df = output_df[['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
        "                       'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n",
        "                       'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n",
        "                       'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']]\n",
        "output_df.to_csv('/content/output.csv', index=False)"
      ],
      "metadata": {
        "id": "_qDCW0bm2UCj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/output.csv')\n",
        "!zip -r /content/text_files.zip /content/*.txt\n",
        "files.download('/content/text_files.zip')\n",
        "\n",
        "with open('/content/instructions.txt', 'w') as f:\n",
        "    f.write('''Approach:\n",
        "- Scraped 147 articles in Colab.\n",
        "- Analyzed each for 13 metrics.\n",
        "- Saved results in output.csv.\n",
        "\n",
        "How to Run:\n",
        "1. Upload Input.xlsx, StopWords, MasterDictionary.\n",
        "2. Run the code.\n",
        "3. Download output.csv and text_files.zip.\n",
        "\n",
        "Dependencies:\n",
        "- pandas, requests (pre-installed)\n",
        "- beautifulsoup4, nltk (installed)''')\n",
        "files.download('/content/instructions.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HQBCpku_6xY2",
        "outputId": "7726e0e2-b1fe-4207-ce98-e7ec3f9d4c30"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5d124d5e-4a66-4656-abb9-b410024a6fc3\", \"output.csv\", 39589)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/Netclan20241017.txt (deflated 45%)\n",
            "  adding: content/Netclan20241018.txt (deflated 60%)\n",
            "  adding: content/Netclan20241019.txt (deflated 47%)\n",
            "  adding: content/Netclan20241020.txt (deflated 59%)\n",
            "  adding: content/Netclan20241021.txt (deflated 46%)\n",
            "  adding: content/Netclan20241022.txt (deflated 45%)\n",
            "  adding: content/Netclan20241023.txt (deflated 60%)\n",
            "  adding: content/Netclan20241024.txt (deflated 47%)\n",
            "  adding: content/Netclan20241025.txt (deflated 59%)\n",
            "  adding: content/Netclan20241026.txt (deflated 65%)\n",
            "  adding: content/Netclan20241027.txt (deflated 55%)\n",
            "  adding: content/Netclan20241028.txt (deflated 58%)\n",
            "  adding: content/Netclan20241029.txt (deflated 50%)\n",
            "  adding: content/Netclan20241030.txt (deflated 53%)\n",
            "  adding: content/Netclan20241031.txt (deflated 61%)\n",
            "  adding: content/Netclan20241032.txt (deflated 47%)\n",
            "  adding: content/Netclan20241033.txt (deflated 58%)\n",
            "  adding: content/Netclan20241034.txt (deflated 45%)\n",
            "  adding: content/Netclan20241035.txt (deflated 51%)\n",
            "  adding: content/Netclan20241036.txt (deflated 51%)\n",
            "  adding: content/Netclan20241037.txt (deflated 53%)\n",
            "  adding: content/Netclan20241038.txt (deflated 54%)\n",
            "  adding: content/Netclan20241039.txt (deflated 52%)\n",
            "  adding: content/Netclan20241040.txt (deflated 53%)\n",
            "  adding: content/Netclan20241041.txt (deflated 54%)\n",
            "  adding: content/Netclan20241042.txt (deflated 51%)\n",
            "  adding: content/Netclan20241043.txt (deflated 52%)\n",
            "  adding: content/Netclan20241044.txt (deflated 56%)\n",
            "  adding: content/Netclan20241045.txt (deflated 52%)\n",
            "  adding: content/Netclan20241046.txt (deflated 57%)\n",
            "  adding: content/Netclan20241047.txt (deflated 49%)\n",
            "  adding: content/Netclan20241048.txt (deflated 59%)\n",
            "  adding: content/Netclan20241049.txt (deflated 60%)\n",
            "  adding: content/Netclan20241050.txt (deflated 51%)\n",
            "  adding: content/Netclan20241051.txt (deflated 69%)\n",
            "  adding: content/Netclan20241052.txt (deflated 58%)\n",
            "  adding: content/Netclan20241053.txt (deflated 59%)\n",
            "  adding: content/Netclan20241054.txt (deflated 52%)\n",
            "  adding: content/Netclan20241055.txt (deflated 66%)\n",
            "  adding: content/Netclan20241056.txt (deflated 39%)\n",
            "  adding: content/Netclan20241057.txt (deflated 52%)\n",
            "  adding: content/Netclan20241058.txt (deflated 49%)\n",
            "  adding: content/Netclan20241059.txt (deflated 51%)\n",
            "  adding: content/Netclan20241060.txt (deflated 54%)\n",
            "  adding: content/Netclan20241061.txt (deflated 55%)\n",
            "  adding: content/Netclan20241062.txt (deflated 48%)\n",
            "  adding: content/Netclan20241063.txt (deflated 48%)\n",
            "  adding: content/Netclan20241064.txt (deflated 36%)\n",
            "  adding: content/Netclan20241065.txt (deflated 35%)\n",
            "  adding: content/Netclan20241066.txt (deflated 41%)\n",
            "  adding: content/Netclan20241067.txt (deflated 49%)\n",
            "  adding: content/Netclan20241068.txt (deflated 50%)\n",
            "  adding: content/Netclan20241069.txt (deflated 41%)\n",
            "  adding: content/Netclan20241070.txt (deflated 58%)\n",
            "  adding: content/Netclan20241071.txt (deflated 55%)\n",
            "  adding: content/Netclan20241072.txt (deflated 43%)\n",
            "  adding: content/Netclan20241073.txt (deflated 51%)\n",
            "  adding: content/Netclan20241074.txt (deflated 53%)\n",
            "  adding: content/Netclan20241075.txt (deflated 44%)\n",
            "  adding: content/Netclan20241076.txt (stored 0%)\n",
            "  adding: content/Netclan20241077.txt (deflated 38%)\n",
            "  adding: content/Netclan20241078.txt (deflated 41%)\n",
            "  adding: content/Netclan20241079.txt (deflated 39%)\n",
            "  adding: content/Netclan20241080.txt (deflated 41%)\n",
            "  adding: content/Netclan20241081.txt (deflated 53%)\n",
            "  adding: content/Netclan20241082.txt (deflated 54%)\n",
            "  adding: content/Netclan20241083.txt (deflated 58%)\n",
            "  adding: content/Netclan20241084.txt (deflated 63%)\n",
            "  adding: content/Netclan20241085.txt (deflated 60%)\n",
            "  adding: content/Netclan20241086.txt (deflated 61%)\n",
            "  adding: content/Netclan20241087.txt (deflated 60%)\n",
            "  adding: content/Netclan20241088.txt (deflated 51%)\n",
            "  adding: content/Netclan20241089.txt (deflated 60%)\n",
            "  adding: content/Netclan20241090.txt (deflated 34%)\n",
            "  adding: content/Netclan20241091.txt (deflated 53%)\n",
            "  adding: content/Netclan20241092.txt (deflated 54%)\n",
            "  adding: content/Netclan20241093.txt (deflated 45%)\n",
            "  adding: content/Netclan20241094.txt (deflated 54%)\n",
            "  adding: content/Netclan20241095.txt (deflated 54%)\n",
            "  adding: content/Netclan20241096.txt (deflated 52%)\n",
            "  adding: content/Netclan20241097.txt (stored 0%)\n",
            "  adding: content/Netclan20241098.txt (deflated 48%)\n",
            "  adding: content/Netclan20241099.txt (deflated 51%)\n",
            "  adding: content/Netclan20241100.txt (deflated 51%)\n",
            "  adding: content/Netclan20241101.txt (deflated 50%)\n",
            "  adding: content/Netclan20241102.txt (deflated 42%)\n",
            "  adding: content/Netclan20241103.txt (deflated 48%)\n",
            "  adding: content/Netclan20241104.txt (deflated 51%)\n",
            "  adding: content/Netclan20241105.txt (deflated 49%)\n",
            "  adding: content/Netclan20241106.txt (deflated 51%)\n",
            "  adding: content/Netclan20241107.txt (deflated 59%)\n",
            "  adding: content/Netclan20241108.txt (deflated 57%)\n",
            "  adding: content/Netclan20241109.txt (deflated 58%)\n",
            "  adding: content/Netclan20241110.txt (deflated 36%)\n",
            "  adding: content/Netclan20241111.txt (deflated 47%)\n",
            "  adding: content/Netclan20241112.txt (deflated 45%)\n",
            "  adding: content/Netclan20241113.txt (deflated 47%)\n",
            "  adding: content/Netclan20241114.txt (deflated 50%)\n",
            "  adding: content/Netclan20241115.txt (deflated 52%)\n",
            "  adding: content/Netclan20241116.txt (deflated 56%)\n",
            "  adding: content/Netclan20241117.txt (deflated 52%)\n",
            "  adding: content/Netclan20241118.txt (deflated 42%)\n",
            "  adding: content/Netclan20241119.txt (deflated 52%)\n",
            "  adding: content/Netclan20241120.txt (deflated 17%)\n",
            "  adding: content/Netclan20241121.txt (deflated 54%)\n",
            "  adding: content/Netclan20241122.txt (deflated 52%)\n",
            "  adding: content/Netclan20241123.txt (deflated 53%)\n",
            "  adding: content/Netclan20241124.txt (deflated 46%)\n",
            "  adding: content/Netclan20241125.txt (deflated 57%)\n",
            "  adding: content/Netclan20241126.txt (deflated 51%)\n",
            "  adding: content/Netclan20241127.txt (deflated 53%)\n",
            "  adding: content/Netclan20241128.txt (deflated 50%)\n",
            "  adding: content/Netclan20241129.txt (deflated 50%)\n",
            "  adding: content/Netclan20241130.txt (deflated 33%)\n",
            "  adding: content/Netclan20241131.txt (deflated 42%)\n",
            "  adding: content/Netclan20241132.txt (deflated 46%)\n",
            "  adding: content/Netclan20241133.txt (deflated 51%)\n",
            "  adding: content/Netclan20241134.txt (deflated 55%)\n",
            "  adding: content/Netclan20241135.txt (deflated 58%)\n",
            "  adding: content/Netclan20241136.txt (deflated 53%)\n",
            "  adding: content/Netclan20241137.txt (deflated 42%)\n",
            "  adding: content/Netclan20241138.txt (deflated 37%)\n",
            "  adding: content/Netclan20241139.txt (deflated 43%)\n",
            "  adding: content/Netclan20241140.txt (deflated 44%)\n",
            "  adding: content/Netclan20241141.txt (deflated 50%)\n",
            "  adding: content/Netclan20241142.txt (deflated 45%)\n",
            "  adding: content/Netclan20241143.txt (deflated 55%)\n",
            "  adding: content/Netclan20241144.txt (deflated 51%)\n",
            "  adding: content/Netclan20241145.txt (deflated 52%)\n",
            "  adding: content/Netclan20241146.txt (deflated 49%)\n",
            "  adding: content/Netclan20241147.txt (deflated 53%)\n",
            "  adding: content/Netclan20241148.txt (deflated 54%)\n",
            "  adding: content/Netclan20241149.txt (deflated 44%)\n",
            "  adding: content/Netclan20241150.txt (deflated 46%)\n",
            "  adding: content/Netclan20241151.txt (deflated 46%)\n",
            "  adding: content/Netclan20241152.txt (deflated 44%)\n",
            "  adding: content/Netclan20241153.txt (deflated 43%)\n",
            "  adding: content/Netclan20241154.txt (deflated 41%)\n",
            "  adding: content/Netclan20241155.txt (deflated 42%)\n",
            "  adding: content/Netclan20241156.txt (deflated 46%)\n",
            "  adding: content/Netclan20241157.txt (deflated 61%)\n",
            "  adding: content/Netclan20241158.txt (deflated 60%)\n",
            "  adding: content/Netclan20241159.txt (deflated 58%)\n",
            "  adding: content/Netclan20241160.txt (deflated 64%)\n",
            "  adding: content/Netclan20241161.txt (deflated 46%)\n",
            "  adding: content/Netclan20241162.txt (deflated 23%)\n",
            "  adding: content/Netclan20241163.txt (deflated 44%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_82697eb9-6ecd-420f-8577-32bd57901996\", \"text_files.zip\", 176813)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_73675067-f488-4d95-9569-b4c96ef119d0\", \"instructions.txt\", 314)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}